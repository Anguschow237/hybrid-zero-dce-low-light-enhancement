{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anguschow237/hybrid-zero-dce-low-light-enhancement/blob/main/hybrid_zero_dce_low_light_image_enhancement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTTlo6vEldei"
      },
      "source": [
        "# Low-Light Image Enhancement: U-Net vs. ZeroDCE Comparison\n",
        "\n",
        "### **Author:** Chow Tsz Hin  \n",
        "### **Date:** 20th December 2025  \n",
        "\n",
        "This project benchmarks four deep learning models for low-light image enhancement:\n",
        "- Supervised U-Net with Charbonnier loss\n",
        "- Unsupervised ZeroDCE (original)\n",
        "- Hybrid ZeroDCE (original loss + Charbonnier)\n",
        "- Supervised ZeroDCE (Charbonnier only)\n",
        "\n",
        "Key experiments include loss function comparisons and hybrid approaches to improve unsupervised performance.\n",
        "\n",
        "**Content Sections** (collapsible for easy navigation):\n",
        "\n",
        "1. Setting up the environment  \n",
        "2. Supervised Loss Function Comparisons  \n",
        "3. Unsupervised ZeroDCE Baseline\n",
        "4. Modifying ZeroDCE for supervised/hybrid training  \n",
        "5. Comparison of the four models (quantitative + qualitative)  \n",
        "6. Final analysis and insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z1J8wEAZpBm"
      },
      "source": [
        "## üìå 1. Setting up the environment & Import the LOL Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZR0cD7A4AY3"
      },
      "outputs": [],
      "source": [
        "  # Run this first\n",
        "  !pip install -q piqa lpips kornia gdown\n",
        "\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F\n",
        "  import torchvision.transforms as T\n",
        "  from torch.utils.data import Dataset, DataLoader\n",
        "  from pathlib import Path\n",
        "  from PIL import Image\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  from tqdm import tqdm\n",
        "  import random\n",
        "  import os\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4IGHy2-Ura5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/lol_dataset /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeYQ2Khf-ssK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "low_dir  = Path(\"/content/lol_dataset/our485/low\")\n",
        "high_dir = Path(\"/content/lol_dataset/our485/high\")\n",
        "\n",
        "print(\"First 20 files in low/ : \", sorted(low_dir.glob(\"*.png\"))[:20])\n",
        "print(\"First 20 files in high/: \", sorted(high_dir.glob(\"*.png\"))[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW-KaS0qZl3S"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths after manual unzip (exact structure from Kaggle)\n",
        "low_path = \"/content/lol_dataset/our485/low/75.png\"\n",
        "high_path = \"/content/lol_dataset/our485/high/75.png\"\n",
        "\n",
        "low_img = Image.open(low_path)\n",
        "high_img = Image.open(high_path)\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Low-Light Input (Very Dark)\", fontsize=18, color=\"red\")\n",
        "plt.imshow(low_img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Ground Truth (Normal Light)\", fontsize=18, color=\"green\")\n",
        "plt.imshow(high_img)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Success! Dataset loaded ‚Äì see the dark vs. bright pair above.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztsdmM_I4JsB"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "class LOLDataset(Dataset):\n",
        "    def __init__(self, root=\"/content/lol_dataset\", train=True, crop_size=256):\n",
        "        self.train = train\n",
        "        self.crop_size = crop_size\n",
        "        low_dir  = Path(root) / \"our485\" / \"low\"\n",
        "        high_dir = Path(root) / \"our485\" / \"high\"\n",
        "        self.lows  = sorted(low_dir.glob(\"*.png\"))\n",
        "        self.highs = sorted(high_dir.glob(\"*.png\"))\n",
        "\n",
        "    def __len__(self): return len(self.lows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        low  = Image.open(self.lows[idx]).convert(\"RGB\")\n",
        "        high = Image.open(self.highs[idx]).convert(\"RGB\")\n",
        "\n",
        "        if self.train:\n",
        "            i, j, h, w = T.RandomCrop.get_params(low, (self.crop_size, self.crop_size))\n",
        "            low  = low.crop((j, i, j+w, i+h))\n",
        "            high = high.crop((j, i, j+w, i+h))\n",
        "            if random.random() > 0.5:\n",
        "                low  = low.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "                high = high.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        return T.ToTensor()(low), T.ToTensor()(high)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rThdQdvkufuk"
      },
      "source": [
        "# üìå 2. Supervised Loss Function Comparisons\n",
        "\n",
        "I started with the U-Net architecture, as it is well-suited for pixel-level image-to-image tasks like low-light enhancement due to its encoder-decoder structure and skip connections that preserve fine details.\n",
        "\n",
        "To determine the most effective loss function for generating bright, natural-looking enhanced images that closely match the ground truth, I systematically compared four common reconstruction losses:\n",
        "\n",
        "- **L1 Loss**: Simple pixel-wise absolute difference; produced decent results but often led to slight blurring and inconsistent brightness.\n",
        "- **SSIM Loss**: Focuses on structural similarity; tended to collapse to overly dark outputs, failing to adequately boost illumination.\n",
        "- **L1 + SSIM Combination**: Aimed to balance pixel accuracy and perceptual quality, but training was unstable, with SSIM dominating and resulting in darkened images.\n",
        "- **Charbonnier Loss**: A robust differentiable variant of L1 (handles outliers better); achieved the best stability, highest PSNR/SSIM scores, and visually superior results with accurate brightness, color fidelity, and detail preservation.\n",
        "\n",
        "Charbonnier loss was selected as the optimal choice for the supervised U-Net baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3usApstquLR"
      },
      "outputs": [],
      "source": [
        "from piqa import SSIM\n",
        "\n",
        "class L1Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, enhanced, target):\n",
        "        return F.l1_loss(enhanced, target)\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ssim = SSIM()\n",
        "\n",
        "    def forward(self, enhanced, target):\n",
        "        return 1 - self.ssim(enhanced, target)\n",
        "\n",
        "class ComboLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = L1Loss()\n",
        "        self.ssim_loss = SSIMLoss()\n",
        "\n",
        "    def forward(self, enhanced, target):\n",
        "        return 0.8 * self.l1(enhanced, target) + 0.2 * self.ssim_loss(enhanced, target)\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, enhanced, target):\n",
        "        diff = enhanced - target\n",
        "        return torch.mean(torch.sqrt(diff * diff + self.eps * self.eps))\n",
        "\n",
        "# List of losses to experiment with\n",
        "# Explicitly move ALL models to the correct device to avoid errors\n",
        "losses_to_test = {\n",
        "    'L1': L1Loss().to(device),\n",
        "    'SSIM': SSIMLoss().to(device),\n",
        "    'Combo': ComboLoss().to(device),\n",
        "    'Charbonnier': CharbonnierLoss().to(device)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb6oRny1rwn0"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# These settings fully utilize the A100 (40‚Äì80 GB) or V100 (32 GB) you now have\n",
        "MODEL_DEPTH  = 6\n",
        "BATCH_SIZE   = 32\n",
        "CROP_SIZE    = 320\n",
        "EPOCHS       = 50\n",
        "LR           = 2e-4\n",
        "SHOW_EVERY   = 5\n",
        "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "# Optional but highly recommended: Mixed Precision (cuts memory ~50%, speeds up ~2√ó)\n",
        "# Updated to use torch.amp to avoid deprecation warnings\n",
        "\n",
        "# =============================================================================\n",
        "# üß† MODEL ARCHITECTURE (The \"Brain\")\n",
        "# This is the U-Net structure required to run the training.\n",
        "# It is named 'UNet' here as the model variable name.\n",
        "# =============================================================================\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, depth=6):\n",
        "        super(UNet, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        self.dec_layers = nn.ModuleList()\n",
        "        self.up_layers = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        # Encoder\n",
        "        ch = 32\n",
        "        for _ in range(depth):\n",
        "            self.enc_layers.append(self.conv_block(in_channels, ch))\n",
        "            in_channels = ch\n",
        "            ch *= 2\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(in_channels, ch)\n",
        "\n",
        "        # Decoder\n",
        "        for _ in range(depth):\n",
        "            self.up_layers.append(nn.ConvTranspose2d(ch, ch//2, kernel_size=2, stride=2))\n",
        "            ch //= 2\n",
        "            self.dec_layers.append(self.conv_block(ch*2, ch))\n",
        "\n",
        "        self.final = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_c, out_c):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_c),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skips = []\n",
        "        # Encoder\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer(x)\n",
        "            skips.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder\n",
        "        for i in range(self.depth):\n",
        "            x = self.up_layers[i](x)\n",
        "            # Handle cropping if sizes don't match exactly due to odd dimensions\n",
        "            if x.shape != skips[-(i+1)].shape:\n",
        "                x = T.Resize(skips[-(i+1)].shape[2:])(x)\n",
        "            x = torch.cat([x, skips[-(i+1)]], dim=1)\n",
        "            x = self.dec_layers[i](x)\n",
        "\n",
        "        # === FIX: Apply Sigmoid to force output to [0, 1] range ===\n",
        "        return torch.sigmoid(self.final(x))\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING FUNCTION\n",
        "# =============================================================================\n",
        "def train_model(loss_name, criterion,\n",
        "                epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                crop_size=CROP_SIZE, lr=LR, model_depth=MODEL_DEPTH, show_every=SHOW_EVERY):\n",
        "\n",
        "    train_dataset = LOLDataset(train=True, crop_size=crop_size)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=4, pin_memory=True, prefetch_factor=2)\n",
        "\n",
        "    # We instantiate the model here\n",
        "    model = UNet(depth=model_depth).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    # Updated AMP Scaler\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for low, high in train_loader:\n",
        "            low, high = low.to(device), high.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Updated autocast context\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                enhanced = model(low)\n",
        "                loss = criterion(enhanced, high)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # stable training\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Only print and show results every 'show_every' epochs\n",
        "        if epoch % show_every == 0:\n",
        "            print(f\"Epoch {epoch:2d}/{epochs} ({loss_name}) ‚Üí Loss: {avg_loss:.6f}\")\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                idx = random.randint(0, len(all_low_paths)-1)\n",
        "                low_img = Image.open(all_low_paths[idx]).convert(\"RGB\")\n",
        "                high_img = Image.open(all_high_paths[idx]).convert(\"RGB\") # Load Ground Truth\n",
        "\n",
        "                input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    enhanced_tensor = model(input_tensor).clamp(0,1).squeeze(0).cpu()\n",
        "                enhanced_img = T.ToPILImage()(enhanced_tensor)\n",
        "\n",
        "                plt.figure(figsize=(15,5))\n",
        "                plt.subplot(1,3,1); plt.title(\"Low-light Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "                plt.subplot(1,3,2); plt.title(\"Ground Truth\");    plt.imshow(high_img); plt.axis('off')\n",
        "                plt.subplot(1,3,3); plt.title(f\"Enhanced ‚Äì {loss_name}\"); plt.imshow(enhanced_img); plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            model.train()\n",
        "\n",
        "    # Save\n",
        "    save_name = f\"zero_dce_{loss_name.lower()}_pro.pth\"\n",
        "    torch.save(model.state_dict(), save_name)\n",
        "    print(f\"Model saved: {save_name}\")\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(losses); plt.title(f\"Training Loss ({loss_name})\"); plt.grid(); plt.show()\n",
        "\n",
        "    return model, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Keb7oNQSr1yC"
      },
      "outputs": [],
      "source": [
        "# Define full paths once (run this once)\n",
        "all_low_paths  = sorted((Path(\"/content/lol_dataset\") / \"our485\" / \"low\").glob(\"*.png\"))\n",
        "all_high_paths = sorted((Path(\"/content/lol_dataset\") / \"our485\" / \"high\").glob(\"*.png\"))\n",
        "\n",
        "# Train all four losses\n",
        "trained_models = {}\n",
        "all_train_losses = {}\n",
        "\n",
        "for loss_name, criterion in losses_to_test.items():\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"STARTING TRAINING WITH {loss_name.upper()} LOSS\")\n",
        "    print(\"=\"*60)\n",
        "    model, losses = train_model(\n",
        "        loss_name=loss_name,\n",
        "        criterion=criterion,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        crop_size=CROP_SIZE,\n",
        "        lr=1e-4,\n",
        "        model_depth=MODEL_DEPTH,\n",
        "        show_every=SHOW_EVERY\n",
        "    )\n",
        "    trained_models[loss_name] = model\n",
        "    all_train_losses[loss_name] = losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKY2lEQiKYme"
      },
      "source": [
        "## Evaluation Function and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMjtAegRr4W4"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_models(models_dict):\n",
        "    test_dataset = LOLDataset(train=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    results = {name: {'PSNR': [], 'SSIM': []} for name in models_dict}\n",
        "\n",
        "    for low, high in test_loader:\n",
        "        low, high = low.to(device), high.to(device)\n",
        "        high_cpu = high.squeeze(0).cpu().numpy().transpose(1,2,0)  # For PSNR\n",
        "\n",
        "        for name, model in models_dict.items():\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                enhanced = model(low).clamp(0,1).squeeze(0)\n",
        "                enhanced_cpu = enhanced.cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "                psnr_val = psnr(high_cpu, enhanced_cpu, data_range=1.0)\n",
        "                # Ensure SSIM module is on the same device as tensors\n",
        "                ssim_val = SSIM().to(device)(enhanced.unsqueeze(0), high).item()\n",
        "\n",
        "                results[name]['PSNR'].append(psnr_val)\n",
        "                results[name]['SSIM'].append(ssim_val)\n",
        "\n",
        "    # Average metrics\n",
        "    avg_results = {name: {'Avg PSNR': np.mean(vals['PSNR']), 'Avg SSIM': np.mean(vals['SSIM'])} for name, vals in results.items()}\n",
        "\n",
        "    # Display as table\n",
        "    df = pd.DataFrame(avg_results).T\n",
        "    print(\"Evaluation Results on Test Set:\")\n",
        "    display(df)\n",
        "\n",
        "    # Find best loss\n",
        "    best_psnr = df['Avg PSNR'].idxmax()\n",
        "    best_ssim = df['Avg SSIM'].idxmax()\n",
        "    print(f\"Best by PSNR: {best_psnr}\")\n",
        "    print(f\"Best by SSIM: {best_ssim}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Run evaluation\n",
        "eval_df = evaluate_models(trained_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v2y3XDEr8AJ"
      },
      "outputs": [],
      "source": [
        "# Pick a random test image and show enhancements from all models\n",
        "test_low_paths = sorted((Path(\"/content/lol_dataset/eval15/low\")).glob(\"*.png\"))\n",
        "test_high_paths = sorted((Path(\"/content/lol_dataset/eval15/high\")).glob(\"*.png\"))\n",
        "\n",
        "if len(test_low_paths) == 0:\n",
        "    print(\"‚ö†Ô∏è Error: No images found in /content/lol_dataset/eval15/low\")\n",
        "    print(\"Please check if the dataset was copied correctly. You might need to re-run the dataset setup cell.\")\n",
        "else:\n",
        "    # Use dynamic length to avoid IndexError\n",
        "    idx = random.randint(0, len(test_low_paths) - 1)\n",
        "    print(f\"Testing on image index: {idx} / {len(test_low_paths)-1}\")\n",
        "\n",
        "    low_img = Image.open(test_low_paths[idx]).convert(\"RGB\")\n",
        "    high_img = Image.open(test_high_paths[idx]).convert(\"RGB\")\n",
        "    input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.subplot(1, len(trained_models)+2, 1); plt.title(\"Low-light Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "    plt.subplot(1, len(trained_models)+2, 2); plt.title(\"Ground Truth\"); plt.imshow(high_img); plt.axis('off')\n",
        "\n",
        "    for i, (name, model) in enumerate(trained_models.items(), start=3):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Ensure output is clamped to [0,1] to avoid display errors\n",
        "            enhanced = model(input_tensor).clamp(0,1).squeeze(0).cpu()\n",
        "            enhanced_img = T.ToPILImage()(enhanced)\n",
        "        plt.subplot(1, len(trained_models)+2, i); plt.title(f\"Enhanced ({name})\"); plt.imshow(enhanced_img); plt.axis('off')\n",
        "\n",
        "    plt.suptitle(\"Comparison on a Test Image\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DgxMVVCkleq"
      },
      "source": [
        "## Quantitative Evaluation of Supervised Loss Functions (Test Set)\n",
        "\n",
        "Direct comparison of training/validation loss values is not meaningful, as each loss optimizes a different objective.\n",
        "\n",
        "Instead, we use standard reference-based metrics to fairly assess reconstruction quality:\n",
        "\n",
        "- **PSNR** (Peak Signal-to-Noise Ratio): Higher ‚Üí better pixel-level accuracy  \n",
        "- **SSIM** (Structural Similarity Index): Higher (closer to 1.0) ‚Üí better perceptual quality\n",
        "\n",
        "### Average Results Across the Test Set\n",
        "\n",
        "| Loss Function       | Avg PSNR   | Avg SSIM   |\n",
        "|---------------------|------------|------------|\n",
        "| L1                  | 19.594     | 0.766      |\n",
        "| SSIM                | 10.783     | 0.504      |\n",
        "| L1 + SSIM Combo     | 10.752     | 0.530      |\n",
        "| Charbonnier         | 19.534     | **0.769**  |\n",
        "\n",
        "### Key Insights\n",
        "- **L1** and **Charbonnier** clearly outperform the others, achieving ~19.5 dB PSNR ‚Äî strong results for low-light enhancement on the LOL dataset.\n",
        "- **Charbonnier** delivers the highest average SSIM (0.769) and, in approximately 70% of test images (by manual inspection), outperforms L1 in **both PSNR and SSIM** simultaneously.\n",
        "- **SSIM-only** and **Combo** suffer severe performance degradation, confirming their tendency to collapse to dark outputs when used without a strong pixel-level intensity term.\n",
        "\n",
        "### Conclusion\n",
        "While L1 achieves a slightly higher average PSNR, **Charbonnier loss** provides the best overall performance. It consistently produces more natural-looking results with superior perceptual quality (highest SSIM) and wins both metrics on the majority of test images. Therefore, Charbonnier loss was selected as the primary supervised loss for the U-Net baseline in subsequent comparisons with ZeroDCE variants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySW0driGlCeK"
      },
      "source": [
        "# üìå 3. Unsupervised ZeroDCE Baseline\n",
        "\n",
        "The ZeroDCE model was implemented based on the original architecture and non-reference loss from the official repository.\n",
        "\n",
        "Minor modifications were made for better training monitoring (progress logging and visualization).\n",
        "\n",
        "Hyperparameters were adopted from the original paper/reference implementation, as extensive testing showed they provided optimal convergence for this unsupervised setting on the LOL dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29c810df"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "#import pytorch_colors as colors\n",
        "import numpy as np\n",
        "\n",
        "# Confirmed: This is the ZeroDCE loss component code from GitHub.\n",
        "class ZeroDCE_Unsupervised_Model(nn.Module):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(ZeroDCE_Unsupervised_Model, self).__init__()\n",
        "\n",
        "\t\tself.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\t\tnumber_f = 32\n",
        "\t\tself.e_conv1 = nn.Conv2d(3,number_f,3,1,1,bias=True)\n",
        "\t\tself.e_conv2 = nn.Conv2d(number_f,number_f,3,1,1,bias=True)\n",
        "\t\tself.e_conv3 = nn.Conv2d(number_f,number_f,3,1,1,bias=True)\n",
        "\t\tself.e_conv4 = nn.Conv2d(number_f,number_f,3,1,1,bias=True)\n",
        "\n",
        "\t\tself.e_conv5 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True)\n",
        "\t\tself.e_conv6 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True)\n",
        "\t\tself.e_conv7 = nn.Conv2d(number_f*2,24,3,1,1,bias=True)\n",
        "\n",
        "\t\tself.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n",
        "\t\tself.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\n",
        "\t\tx1 = self.relu(self.e_conv1(x))\n",
        "\t\t# p1 = self.maxpool(x1)\n",
        "\t\tx2 = self.relu(self.e_conv2(x1))\n",
        "\t\t# p2 = self.maxpool(x2)\n",
        "\t\tx3 = self.relu(self.e_conv3(x2))\n",
        "\t\t# p3 = self.maxpool(x3)\n",
        "\t\tx4 = self.relu(self.e_conv4(x3))\n",
        "\n",
        "\t\tx5 = self.relu(self.e_conv5(torch.cat([x3,x4],1)))\n",
        "\t\t# x5 = self.upsample(x5)\n",
        "\t\tx6 = self.relu(self.e_conv6(torch.cat([x2,x5],1)))\n",
        "\n",
        "\t\tx_r = F.tanh(self.e_conv7(torch.cat([x1,x6],1)))\n",
        "\t\tr1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, 3, dim=1)\n",
        "\n",
        "\n",
        "\t\tx = x + r1*(torch.pow(x,2)-x)\n",
        "\t\tx = x + r2*(torch.pow(x,2)-x)\n",
        "\t\tx = x + r3*(torch.pow(x,2)-x)\n",
        "\t\tenhance_image_1 = x + r4*(torch.pow(x,2)-x)\n",
        "\t\tx = enhance_image_1 + r5*(torch.pow(enhance_image_1,2)-enhance_image_1)\n",
        "\t\tx = x + r6*(torch.pow(x,2)-x)\n",
        "\t\tx = x + r7*(torch.pow(x,2)-x)\n",
        "\t\tenhance_image = x + r8*(torch.pow(x,2)-x)\n",
        "\t\tr = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],1)\n",
        "\t\treturn enhance_image_1,enhance_image,r\n",
        "\n",
        "print(\"ZeroDCE_Unsupervised_Model class defined with adjusted weights for better quality.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAIEMgAWGfKY"
      },
      "outputs": [],
      "source": [
        "# Confirmed: These are the helper loss modules (L_color, L_spa, L_exp, L_TV, Sa_Loss, perception_loss)\n",
        "# copied directly from the ZeroDCE GitHub implementation.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchvision.models.vgg import vgg16\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class L_color(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(L_color, self).__init__()\n",
        "\n",
        "    def forward(self, x ):\n",
        "\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
        "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
        "        Drg = torch.pow(mr-mg,2)\n",
        "        Drb = torch.pow(mr-mb,2)\n",
        "        Dgb = torch.pow(mb-mg,2)\n",
        "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
        "\n",
        "\n",
        "        return k.mean() # Added .mean() to ensure scalar output\n",
        "\n",
        "\n",
        "class L_spa(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(L_spa, self).__init__()\n",
        "        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
        "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).to(device).unsqueeze(0).unsqueeze(0) # Changed .cuda() to .to(device)\n",
        "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).to(device).unsqueeze(0).unsqueeze(0) # Changed .cuda() to .to(device)\n",
        "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).to(device).unsqueeze(0).unsqueeze(0) # Changed .cuda() to .to(device)\n",
        "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).to(device).unsqueeze(0).unsqueeze(0) # Changed .cuda() to .to(device)\n",
        "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
        "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
        "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
        "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
        "        self.pool = nn.AvgPool2d(4)\n",
        "    def forward(self, org , enhance ):\n",
        "        b,c,h,w = org.shape\n",
        "\n",
        "        org_mean = torch.mean(org,1,keepdim=True)\n",
        "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
        "\n",
        "        org_pool =  self.pool(org_mean)\n",
        "        enhance_pool = self.pool(enhance_mean)\n",
        "\n",
        "        weight_diff =torch.max(torch.FloatTensor([1]).to(device) + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).to(device),torch.FloatTensor([0]).to(device)),torch.FloatTensor([0.5]).to(device)) # Changed .cuda() to .to(device)\n",
        "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).to(device)) ,enhance_pool-org_pool) # Changed .cuda() to .to(device)\n",
        "\n",
        "\n",
        "        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n",
        "        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n",
        "        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n",
        "        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n",
        "\n",
        "        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n",
        "        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n",
        "        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n",
        "        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n",
        "\n",
        "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
        "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
        "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
        "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
        "        E = (D_left + D_right + D_up +D_down)\n",
        "        # E = 25*(D_left + D_right + D_up +D_down)\n",
        "\n",
        "        return E.mean() # Added .mean() to return a scalar\n",
        "class L_exp(nn.Module):\n",
        "\n",
        "    def __init__(self,patch_size,mean_val):\n",
        "        super(L_exp, self).__init__()\n",
        "        # print(1)\n",
        "        self.pool = nn.AvgPool2d(patch_size)\n",
        "        self.mean_val = mean_val\n",
        "    def forward(self, x ):\n",
        "\n",
        "        b,c,h,w = x.shape\n",
        "        x = torch.mean(x,1,keepdim=True)\n",
        "        mean = self.pool(x)\n",
        "\n",
        "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).to(device),2)) # Changed .cuda() to .to(device)\n",
        "        return d\n",
        "\n",
        "class L_TV(nn.Module):\n",
        "    def __init__(self,TVLoss_weight=1):\n",
        "        super(L_TV,self).__init__()\n",
        "        self.TVLoss_weight = TVLoss_weight\n",
        "\n",
        "    def forward(self,x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
        "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
        "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
        "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
        "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
        "\n",
        "class Sa_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Sa_Loss, self).__init__()\n",
        "        # print(1)\n",
        "    def forward(self, x ):\n",
        "        # self.grad = np.ones(x.shape,dtype=np.float32)\n",
        "        b,c,h,w = x.shape\n",
        "        # x_de = x.cpu().detach().numpy()\n",
        "        r,g,b = torch.split(x , 1, dim=1)\n",
        "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
        "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
        "        Dr = r-mr\n",
        "        Dg = g-mg\n",
        "        Db = b-mb\n",
        "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
        "        # print(k)\n",
        "\n",
        "\n",
        "        k = torch.mean(k)\n",
        "        return k\n",
        "\n",
        "class perception_loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(perception_loss, self).__init__()\n",
        "        features = vgg16(pretrained=True).features\n",
        "        self.to_relu_1_2 = nn.Sequential()\n",
        "        self.to_relu_2_2 = nn.Sequential()\n",
        "        self.to_relu_3_3 = nn.Sequential()\n",
        "        self.to_relu_4_3 = nn.Sequential()\n",
        "\n",
        "        for x in range(4):\n",
        "            self.to_relu_1_2.add_module(str(x), features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.to_relu_2_2.add_module(str(x), features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.to_relu_3_3.add_module(str(x), features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.to_relu_4_3.add_module(str(x), features[x])\n",
        "\n",
        "        # don't need the gradients, just want the features\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.to_relu_1_2(x)\n",
        "        h_relu_1_2 = h\n",
        "        h = self.to_relu_2_2(h)\n",
        "        h_relu_2_2 = h\n",
        "        h = self.to_relu_3_3(h)\n",
        "        h_relu_3_3 = h\n",
        "        h = self.to_relu_4_3(h)\n",
        "        h_relu_4_3 = h\n",
        "        # out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
        "        return h_relu_4_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15497071"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from piqa import SSIM\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# =============================================================================\n",
        "# HYPERPARAMETERS (Exactly from the GitHub code you provided)\n",
        "# =============================================================================\n",
        "lowlight_images_path = \"data/train_data/\" # (We replace this with LOLDataset)\n",
        "lr = 0.0001 # CHANGED from 0.00005 to 0.0001\n",
        "weight_decay = 0.0001\n",
        "grad_clip_norm = 0.1\n",
        "num_epochs = 200 # CHANGED from 100 to 200\n",
        "train_batch_size = 8\n",
        "val_batch_size = 4\n",
        "num_workers = 4\n",
        "display_iter = 10\n",
        "snapshot_iter = 10\n",
        "snapshots_folder = \"snapshots/\"\n",
        "load_pretrain = False\n",
        "pretrain_dir = \"snapshots/Epoch99.pth\"\n",
        "\n",
        "# =============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# =============================================================================\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING LOGIC\n",
        "# =============================================================================\n",
        "def train_github_style():\n",
        "    # 1. Setup Device\n",
        "    # os.environ['CUDA_VISIBLE_DEVICES']='0' # handled by Colab runtime\n",
        "\n",
        "    # 2. Model Setup\n",
        "    # DCE_net = model.enhance_net_nopool().cuda()\n",
        "    DCE_net = ZeroDCE_Unsupervised_Model().to(device)\n",
        "    DCE_net.apply(weights_init)\n",
        "\n",
        "    # if config.load_pretrain == True:\n",
        "    #     DCE_net.load_state_dict(torch.load(config.pretrain_dir))\n",
        "\n",
        "    # 3. Data Loader Setup\n",
        "    # train_dataset = dataloader.lowlight_loader(config.lowlight_images_path)\n",
        "    train_dataset = LOLDataset(train=True, crop_size=256) # Assuming 256 crop for 8 batch size fit\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    # Validation loader for monitoring (not in original train.py but needed for us to see progress)\n",
        "    val_dataset = LOLDataset(train=False)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # 4. Loss Components Setup\n",
        "    L_color_module = L_color().to(device)\n",
        "    L_spa_module = L_spa().to(device)\n",
        "    L_exp_module = L_exp(16, 0.6).to(device)\n",
        "    L_TV_module = L_TV().to(device)\n",
        "    # Myloss.L_color(), Myloss.L_spa(), etc.\n",
        "\n",
        "    # 5. Optimizer Setup\n",
        "    optimizer = torch.optim.Adam(DCE_net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    DCE_net.train()\n",
        "\n",
        "    # Stats tracking\n",
        "    history = {'epoch': [], 'loss': [], 'psnr': [], 'ssim': []}\n",
        "    cal_ssim = SSIM().to(device)\n",
        "\n",
        "    print(\"Starting training with GitHub parameters...\")\n",
        "    print(f\"Epochs: {num_epochs}, Batch: {train_batch_size}, LR: {lr}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for iteration, (img_lowlight, _) in enumerate(train_loader): # LOLDataset returns (low, high)\n",
        "            img_lowlight = img_lowlight.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            enhanced_image_1, enhanced_image, A = DCE_net(img_lowlight)\n",
        "\n",
        "            # Loss Calculation (Exact weights from GitHub)\n",
        "            Loss_TV = 200 * L_TV_module(A)\n",
        "            loss_spa = torch.mean(L_spa_module(img_lowlight, enhanced_image))\n",
        "            loss_col = 5 * torch.mean(L_color_module(enhanced_image))\n",
        "            loss_exp = 10 * torch.mean(L_exp_module(enhanced_image))\n",
        "\n",
        "            loss = Loss_TV + loss_spa + loss_col + loss_exp\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(DCE_net.parameters(), grad_clip_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Display iter logic from original code (simplified for notebook)\n",
        "            if ((iteration+1) % display_iter) == 0:\n",
        "                # print(\"Loss at iteration\", iteration+1, \":\", loss.item())\n",
        "                pass\n",
        "\n",
        "        # End of Epoch Evaluation\n",
        "        if ((epoch+1) % snapshot_iter) == 0 or epoch == 0:\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Validation\n",
        "            DCE_net.eval()\n",
        "            psnr_val_list, ssim_val_list = [], []\n",
        "            with torch.no_grad():\n",
        "                for v_low, v_high in val_loader:\n",
        "                    v_low, v_high = v_low.to(device), v_high.to(device)\n",
        "                    _, v_enhanced, _ = DCE_net(v_low)\n",
        "                    v_enhanced = v_enhanced.clamp(0,1)\n",
        "                    ssim_val_list.append(cal_ssim(v_enhanced, v_high).item())\n",
        "                    v_enhanced_np = v_enhanced.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                    v_high_np = v_high.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                    psnr_val_list.append(psnr(v_high_np, v_enhanced_np, data_range=1.0))\n",
        "\n",
        "            avg_psnr = sum(psnr_val_list) / len(psnr_val_list)\n",
        "            avg_ssim = sum(ssim_val_list) / len(ssim_val_list)\n",
        "            history['epoch'].append(epoch+1)\n",
        "            history['loss'].append(avg_loss)\n",
        "            history['psnr'].append(avg_psnr)\n",
        "            history['ssim'].append(avg_ssim)\n",
        "\n",
        "            print(f\"Epoch {epoch+1} Test PSNR: {avg_psnr:.2f}, SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "            # Visualize one random image from validation set\n",
        "            with torch.no_grad():\n",
        "                rand_idx = random.randint(0, len(val_dataset) - 1)\n",
        "                v_low, v_high = val_dataset[rand_idx]\n",
        "\n",
        "                # Add batch dimension and move to device\n",
        "                v_low = v_low.unsqueeze(0).to(device)\n",
        "\n",
        "                _, v_enhanced, _ = DCE_net(v_low)\n",
        "                v_enhanced = v_enhanced.clamp(0,1).cpu()\n",
        "\n",
        "                plt.figure(figsize=(15,5))\n",
        "                plt.subplot(1,3,1); plt.imshow(T.ToPILImage()(v_low.squeeze(0))); plt.title('Input')\n",
        "                plt.subplot(1,3,2); plt.imshow(T.ToPILImage()(v_enhanced.squeeze(0))); plt.title('Enhanced')\n",
        "                plt.subplot(1,3,3); plt.imshow(T.ToPILImage()(v_high)); plt.title('Ground Truth')\n",
        "                plt.show()\n",
        "\n",
        "            DCE_net.train()\n",
        "\n",
        "            # Snapshot saving\n",
        "            # if not os.path.exists(snapshots_folder):\n",
        "            #     os.mkdir(snapshots_folder)\n",
        "            # torch.save(DCE_net.state_dict(), snapshots_folder + \"Epoch\" + str(epoch) + '.pth')\n",
        "\n",
        "    # Plotting Training History\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history['epoch'], history['loss'], label='Loss', color='red')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history['epoch'], history['psnr'], label='PSNR', color='green')\n",
        "    plt.title('Validation PSNR')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(history['epoch'], history['ssim'], label='SSIM', color='blue')\n",
        "    plt.title('Validation SSIM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return trained model and history\n",
        "    return DCE_net, history\n",
        "\n",
        "# Run the training\n",
        "unsupervised_model, unsupervised_history = train_github_style()\n",
        "\n",
        "# Save model to global dict for comparison later\n",
        "trained_models['ZeroDCE_Unsupervised'] = unsupervised_model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3c49d82"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from piqa import SSIM\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def evaluate_and_visualize_model(model, history, model_name):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EVALUATING AND VISUALIZING: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Evaluate on test set\n",
        "    test_dataset = LOLDataset(train=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    model.eval()\n",
        "    cal_ssim = SSIM().to(device) # Ensure SSIM module is on the correct device\n",
        "    with torch.no_grad():\n",
        "        for i, (low, high) in enumerate(test_loader):\n",
        "            low, high = low.to(device), high.to(device)\n",
        "\n",
        "            # ZeroDCE_Unsupervised_Model returns (enhanced_image_1, enhanced_image, A)\n",
        "            _, enhanced, _ = model(low)\n",
        "            enhanced = enhanced.clamp(0, 1)\n",
        "\n",
        "            # For PSNR calculation\n",
        "            high_np = high.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
        "            enhanced_np = enhanced.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "            psnr_values.append(psnr(high_np, enhanced_np, data_range=1.0))\n",
        "            ssim_values.append(cal_ssim(enhanced, high).item())\n",
        "\n",
        "    avg_psnr = np.mean(psnr_values)\n",
        "    avg_ssim = np.mean(ssim_values)\n",
        "\n",
        "    print(f\"Average PSNR on Test Set: {avg_psnr:.2f}\")\n",
        "    print(f\"Average SSIM on Test Set: {avg_ssim:.4f}\")\n",
        "\n",
        "    # 2. Re-display training history plots\n",
        "    if history and history['epoch']:\n",
        "        plt.figure(figsize=(18, 5))\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(history['epoch'], history['loss'], label='Loss', color='red')\n",
        "        plt.title(f'{model_name} - Training Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(history['epoch'], history['psnr'], label='PSNR', color='green')\n",
        "        plt.title(f'{model_name} - Validation PSNR')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(history['epoch'], history['ssim'], label='SSIM', color='blue')\n",
        "        plt.title(f'{model_name} - Validation SSIM')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No training history to display.\")\n",
        "\n",
        "    # 3. Show a sample enhanced image\n",
        "    print(f\"\\nDisplaying a sample enhanced image for {model_name}...\")\n",
        "    test_low_paths = sorted((Path(\"/content/lol_dataset/eval15/low\")).glob(\"*.png\"))\n",
        "    test_high_paths = sorted((Path(\"/content/lol_dataset/eval15/high\")).glob(\"*.png\"))\n",
        "\n",
        "    if len(test_low_paths) == 0:\n",
        "        print(\"‚ö†Ô∏è Error: No images found in /content/lol_dataset/eval15/low\")\n",
        "        print(\"Please check if the dataset was copied correctly.\")\n",
        "        return\n",
        "\n",
        "    idx = random.randint(0, len(test_low_paths) - 1)\n",
        "    low_img_path = test_low_paths[idx]\n",
        "    high_img_path = test_high_paths[idx]\n",
        "\n",
        "    low_img = Image.open(low_img_path).convert(\"RGB\")\n",
        "    high_img = Image.open(high_img_path).convert(\"RGB\")\n",
        "    input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, enhanced_tensor, _ = model(input_tensor)\n",
        "        enhanced_tensor = enhanced_tensor.clamp(0, 1).squeeze(0).cpu()\n",
        "    enhanced_img = T.ToPILImage()(enhanced_tensor)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1); plt.title(\"Low-light Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "    plt.subplot(1, 3, 2); plt.title(\"Ground Truth\"); plt.imshow(high_img); plt.axis('off')\n",
        "    plt.subplot(1, 3, 3); plt.title(f\"Enhanced ({model_name})\"); plt.imshow(enhanced_img); plt.axis('off')\n",
        "    plt.suptitle(\"Sample Image Comparison\")\n",
        "    plt.show()\n",
        "\n",
        "# Call the function for the newly trained unsupervised model\n",
        "evaluate_and_visualize_model(\n",
        "    unsupervised_model,\n",
        "    unsupervised_history,\n",
        "    'ZeroDCE_Unsupervised' # Updated model name for clarity\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYjCg4sFpBDl"
      },
      "source": [
        "## üìå 3. Unsupervised ZeroDCE Baseline\n",
        "\n",
        "The original ZeroDCE model relies on a **non-reference (unsupervised) loss** that does not require paired ground-truth images. Instead, it uses carefully designed priors (spatial consistency, exposure control, illumination smoothness) to guide curve-based enhancement.\n",
        "\n",
        "### Training Observations\n",
        "- Loss decreased rapidly in the first 25 epochs, accompanied by significant gains in PSNR and SSIM.\n",
        "- This correlation confirms that the unsupervised objective effectively aligns with perceptual quality improvements, even without direct supervision.\n",
        "- Final test-set performance typically reached **PSNR > 15 dB** and **SSIM > 0.45**, which is respectable for a fully unsupervised approach on the challenging LOL dataset.\n",
        "\n",
        "### Key Insights\n",
        "While visual outputs often appear plausible (good brightness recovery, reduced noise), quantitative metrics (PSNR/SSIM) remain lower than supervised methods. This is expected: supervised models directly optimize toward ground-truth pixels, whereas ZeroDCE relies solely on hand-crafted priors without access to high-light references during training.\n",
        "\n",
        "This unsupervised baseline highlights the strengths of ZeroDCE's lightweight curve estimation while establishing a reference point for the subsequent hybrid experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Comparison: Supervised vs. Unsupervised Approaches"
      ],
      "metadata": {
        "id": "WPlBF3n2RNR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EVALUATION AND VISUAL COMPARISON OF TWO MODELS\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from piqa import SSIM\n",
        "import os\n",
        "\n",
        "# 1. Load the UNet model with Charbonnier loss\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING UNet MODEL WITH CHARBONNIER LOSS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a clean dictionary JUST for this comparison\n",
        "comparison_models = {}\n",
        "\n",
        "# Ensure the UNet class is defined\n",
        "charbonnier_model = UNet(depth=MODEL_DEPTH).to(device)\n",
        "charbonnier_model_path = \"zero_dce_charbonnier_pro.pth\"\n",
        "\n",
        "# Load UNet_Charbonnier\n",
        "if os.path.exists(charbonnier_model_path):\n",
        "    charbonnier_model.load_state_dict(torch.load(charbonnier_model_path))\n",
        "    charbonnier_model.eval()\n",
        "    # Add to our new comparison dictionary\n",
        "    comparison_models['UNet_Charbonnier'] = charbonnier_model\n",
        "    print(f\"Loaded UNet_Charbonnier model from {charbonnier_model_path}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Error: {charbonnier_model_path} not found.\")\n",
        "\n",
        "# 2. Add ZeroDCE_Unsupervised\n",
        "# Check trained_models first, then fallback to local variable 'unsupervised_model'\n",
        "if 'trained_models' in globals() and 'ZeroDCE_Unsupervised' in trained_models:\n",
        "    comparison_models['ZeroDCE_Unsupervised'] = trained_models['ZeroDCE_Unsupervised']\n",
        "    print(\"Added ZeroDCE_Unsupervised from trained_models dictionary.\")\n",
        "elif 'unsupervised_model' in globals():\n",
        "    comparison_models['ZeroDCE_Unsupervised'] = unsupervised_model\n",
        "    print(\"Added ZeroDCE_Unsupervised from 'unsupervised_model' variable.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: 'ZeroDCE_Unsupervised' model not found. Please train it first.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARING UNet_Charbonnier AND ZeroDCE_Unsupervised MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- Custom Evaluation Loop (Self-Contained) ---\n",
        "if not comparison_models:\n",
        "    print(\"‚ö†Ô∏è Error: No models available for comparison.\")\n",
        "else:\n",
        "    # Setup data\n",
        "    test_dataset = LOLDataset(train=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    results = {name: {'PSNR': [], 'SSIM': []} for name in comparison_models}\n",
        "    cal_ssim = SSIM().to(device)\n",
        "\n",
        "    print(f\"Evaluating {len(comparison_models)} models on test set...\")\n",
        "\n",
        "    for low, high in test_loader:\n",
        "        low, high = low.to(device), high.to(device)\n",
        "        high_cpu = high.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "        for name, model in comparison_models.items():\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Get output\n",
        "                output = model(low)\n",
        "\n",
        "                # Handle tuple output for ZeroDCE vs Tensor for UNet\n",
        "                if isinstance(output, tuple):\n",
        "                    _, enhanced_out, _ = output\n",
        "                    enhanced = enhanced_out.clamp(0,1).squeeze(0)\n",
        "                else:\n",
        "                    enhanced = output.clamp(0,1).squeeze(0)\n",
        "\n",
        "                # Metrics\n",
        "                enhanced_cpu = enhanced.cpu().numpy().transpose(1,2,0)\n",
        "                psnr_val = psnr(high_cpu, enhanced_cpu, data_range=1.0)\n",
        "                ssim_val = cal_ssim(enhanced.unsqueeze(0), high).item()\n",
        "\n",
        "                results[name]['PSNR'].append(psnr_val)\n",
        "                results[name]['SSIM'].append(ssim_val)\n",
        "\n",
        "    # Average metrics\n",
        "    avg_results = {name: {'Avg PSNR': np.mean(vals['PSNR']), 'Avg SSIM': np.mean(vals['SSIM'])} for name, vals in results.items()}\n",
        "    df = pd.DataFrame(avg_results).T\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    display(df)\n",
        "\n",
        "    best_psnr = df['Avg PSNR'].idxmax()\n",
        "    best_ssim = df['Avg SSIM'].idxmax()\n",
        "    print(f\"Best by PSNR: {best_psnr}\")\n",
        "    print(f\"Best by SSIM: {best_ssim}\")\n",
        "\n",
        "    # --- Visual Comparison ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VISUAL COMPARISON OF MODELS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_low_paths_eval15 = sorted((Path(\"/content/lol_dataset/eval15/low\")).glob(\"*.png\"))\n",
        "    test_high_paths_eval15 = sorted((Path(\"/content/lol_dataset/eval15/high\")).glob(\"*.png\"))\n",
        "\n",
        "    if len(test_low_paths_eval15) == 0:\n",
        "        print(\"‚ö†Ô∏è Error: No images found in /content/lol_dataset/eval15/low.\")\n",
        "    else:\n",
        "        idx = random.randint(0, len(test_low_paths_eval15) - 1)\n",
        "        print(f\"Testing on image index: {idx} / {len(test_low_paths_eval15)-1}\")\n",
        "\n",
        "        low_img = Image.open(test_low_paths_eval15[idx]).convert(\"RGB\")\n",
        "        high_img = Image.open(test_high_paths_eval15[idx]).convert(\"RGB\")\n",
        "        input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        total_plots = len(comparison_models) + 2\n",
        "\n",
        "        plt.subplot(1, total_plots, 1); plt.title(\"Low-light Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "        plt.subplot(1, total_plots, 2); plt.title(\"Ground Truth\"); plt.imshow(high_img); plt.axis('off')\n",
        "\n",
        "        plot_idx = 3\n",
        "        for name, model in comparison_models.items():\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                output = model(input_tensor)\n",
        "                if isinstance(output, tuple):\n",
        "                     _, enhanced_out, _ = output\n",
        "                     enhanced = enhanced_out.clamp(0,1).squeeze(0).cpu()\n",
        "                else:\n",
        "                     enhanced = output.clamp(0,1).squeeze(0).cpu()\n",
        "\n",
        "                enhanced_img = T.ToPILImage()(enhanced)\n",
        "                plt.subplot(1, total_plots, plot_idx)\n",
        "                plt.title(f\"Enhanced ({name})\")\n",
        "                plt.imshow(enhanced_img)\n",
        "                plt.axis('off')\n",
        "                plot_idx += 1\n",
        "\n",
        "        plt.suptitle(\"Comparison on a Test Image (eval15 Dataset)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "4PYAgqcRgnWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of the two models:\n",
        "\n",
        "The supervised U-Net (with Charbonnier loss) leverages paired ground-truth images, enabling direct pixel-level optimization. This results in superior reconstruction accuracy, natural brightness recovery, and high PSNR/SSIM scores.\n",
        "\n",
        "In contrast, the original ZeroDCE operates fully unsupervised, relying solely on non-reference priors (spatial consistency, exposure control, illumination smoothness). While effective at brightening images without paired data, it exhibits lower quantitative performance and occasional color shifts or residual noise.\n",
        "\n",
        "This gap motivated the next phase: exploring whether incorporating supervised signals into the ZeroDCE framework could bridge the performance difference while retaining its architectural advantages."
      ],
      "metadata": {
        "id": "sDiTpzGvqeGH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJHnTKJrBxp1"
      },
      "source": [
        "# üìå 4. Hybrid and Supervised ZeroDCE Variants\n",
        "\n",
        "To investigate the impact of supervision on the ZeroDCE architecture, two variants were developed:\n",
        "\n",
        "- **Hybrid ZeroDCE (Dual Loss)**:  \n",
        "  Combines the original unsupervised ZeroDCE loss with Charbonnier loss. This approach retains perceptual priors while adding direct reconstruction guidance.\n",
        "\n",
        "- **Supervised ZeroDCE (Charbonnier Only)**:  \n",
        "  Replaces the ZeroDCE loss entirely with Charbonnier loss, converting the model into a fully supervised learner.\n",
        "\n",
        "These experiments aim to isolate the contributions of architecture vs. training objective and explore hybrid strategies for improved stability and quality."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 1: Hybrid_ZeroDCE_Two_Losses\n",
        "\n",
        "This model uses ZeroDCE architecutre with ZeroDCE loss and Charbonnier loss"
      ],
      "metadata": {
        "id": "6BSpaU2ggyHt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35b369d4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from piqa import SSIM\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# =============================================================================\n",
        "# üìå 4. Modify the original unsupervised ZeroDCE model to make it supervised (TUNING)\n",
        "# =============================================================================\n",
        "\n",
        "# 1. Define a new class named ZeroDCE_Supervised_Architecture\n",
        "class ZeroDCE_Supervised_Architecture(ZeroDCE_Unsupervised_Model):\n",
        "    def __init__(self):\n",
        "        super(ZeroDCE_Supervised_Architecture, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, enhanced_image, _ = super().forward(x) # Call parent's forward, take second return\n",
        "        return enhanced_image\n",
        "\n",
        "# Define the missing L_color_supervised (adapted from your commented version to be supervised)\n",
        "class L_color_supervised(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(L_color_supervised, self).__init__()\n",
        "\n",
        "    def forward(self, x, y):  # x: enhanced, y: ground truth\n",
        "        # Compute color constancy for enhanced (as in original)\n",
        "        r, g, b = torch.split(x, 1, dim=1)\n",
        "        mean_rgb_x = torch.mean(x, [2, 3], keepdim=True)\n",
        "        mr_x, mg_x, mb_x = torch.split(mean_rgb_x, 1, dim=1)\n",
        "        Dr_x = r - mr_x\n",
        "        Dg_x = g - mg_x\n",
        "        Db_x = b - mb_x\n",
        "        k_x = torch.pow(torch.pow(Dr_x, 2) + torch.pow(Dg_x, 2) + torch.pow(Db_x, 2) + 1e-8, 0.5)\n",
        "        k_x = torch.mean(k_x)\n",
        "\n",
        "        # Compute the same for ground truth (for supervision)\n",
        "        r_y, g_y, b_y = torch.split(y, 1, dim=1)\n",
        "        mean_rgb_y = torch.mean(y, [2, 3], keepdim=True)\n",
        "        mr_y, mg_y, mb_y = torch.split(mean_rgb_y, 1, dim=1)\n",
        "        Dr_y = r_y - mr_y\n",
        "        Dg_y = g_y - mg_y\n",
        "        Db_y = b_y - mb_y\n",
        "        k_y = torch.pow(torch.pow(Dr_y, 2) + torch.pow(Db_y, 2) + torch.pow(Dg_y, 2) + 1e-8, 0.5)\n",
        "        k_y = torch.mean(k_y)\n",
        "\n",
        "        # Supervised: Encourage enhanced to match ground truth's color constancy (absolute diff)\n",
        "        return torch.abs(k_x - k_y)\n",
        "\n",
        "# Reuse CharbonnierLoss (already defined in your notebook)\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (reused from your supervised U-Net section)\"\"\"\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        return torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))\n",
        "\n",
        "# Hybrid Loss Class (as before, now with defined L_color_supervised and adjustable weights)\n",
        "class ZeroDCELoss_Hybrid_Supervised(nn.Module):\n",
        "    \"\"\"Hybrid Supervised ZeroDCE Loss: Reuses original components + adds Charbonnier for direct supervision\"\"\"\n",
        "    def __init__(self, recon_weight, percep_weight, spa_weight, tv_weight, col_weight, exp_weight, sa_weight):\n",
        "        super(ZeroDCELoss_Hybrid_Supervised, self).__init__()\n",
        "        # Store weights as instance attributes\n",
        "        self.recon_weight = recon_weight\n",
        "        self.percep_weight = percep_weight\n",
        "        self.spa_weight = spa_weight\n",
        "        self.tv_weight = tv_weight\n",
        "        self.col_weight = col_weight\n",
        "        self.exp_weight = exp_weight\n",
        "        self.sa_weight = sa_weight\n",
        "\n",
        "        # Reuse existing losses from your notebook\n",
        "        self.L_color = L_color_supervised()  # Now defined above\n",
        "        self.L_spatial = L_spa()             # Defined\n",
        "        self.L_exp = L_exp(16, 0.6)          # Defined\n",
        "        self.L_tv = L_TV()                   # Defined\n",
        "        self.L_percep = perception_loss().to(device)  # Defined\n",
        "        self.sa_loss = Sa_Loss()             # Defined\n",
        "        self.charbonnier = CharbonnierLoss() # Reuse\n",
        "\n",
        "    def forward(self, enhanced, high_img):\n",
        "        # --- Reuse Original/Adapted ZeroDCE Terms (as priors) ---\n",
        "        loss_percep = self.percep_weight * torch.mean(torch.abs(self.L_percep(enhanced) - self.L_percep(high_img)))\n",
        "        loss_spa = self.spa_weight * self.L_spatial(high_img, enhanced)  # Supervised spatial\n",
        "        loss_tv = self.tv_weight * self.L_tv(enhanced)\n",
        "        loss_col = self.col_weight * self.L_color(enhanced, high_img)    # Now supervised\n",
        "        loss_exp = self.exp_weight * self.L_exp(enhanced)                # Unsupervised exposure prior\n",
        "        loss_sa = self.sa_weight * self.sa_loss(enhanced)                # Unsupervised saturation prior\n",
        "\n",
        "        # --- New: Add Supervised Reconstruction Term (Charbonnier) ---\n",
        "        loss_recon = self.recon_weight * self.charbonnier(enhanced, high_img)  # New weight for tuning\n",
        "\n",
        "        # Total Hybrid Loss\n",
        "        return loss_percep + loss_spa + loss_tv + loss_col + loss_exp + loss_sa + loss_recon\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING: Reuse most of the loop from Section 3, but with hybrid loss\n",
        "# =============================================================================\n",
        "\n",
        "# Define new hyperparameters for the hybrid loss (Modified for equal contribution)\n",
        "recon_weight = 1.0 # Reduced from 100.0 to balance with priors\n",
        "percep_weight = 0.28 # Scaled down from 1.0 (1.0 / 3.52)\n",
        "spa_weight = 0.28    # Scaled down from 1.0 (1.0 / 3.52)\n",
        "tv_weight = 0.28     # Scaled down from 1.0 (1.0 / 3.52)\n",
        "col_weight = 0.14    # Scaled down from 0.5 (0.5 / 3.52)\n",
        "exp_weight = 0.003   # Scaled down from 0.01 (0.01 / 3.52)\n",
        "sa_weight = 0.003    # Scaled down from 0.01 (0.01 / 3.52)\n",
        "\n",
        "# Adjust training hyperparameters for this model to match paper\n",
        "LR = 0.0001 # CHANGED from 5e-5 to 0.0001\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 8 # CHANGED from 16 to 8\n",
        "CROP_SIZE = 256\n",
        "SHOW_EVERY = 10\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING: SUPERVISED ZERODCE (HYBRID LOSS) - Balanced Contribution\")\n",
        "print(f\"üîß CURRENT WEIGHTS: Recon={recon_weight}, Perceptual={percep_weight}, Spatial={spa_weight}, TV={tv_weight}, Color={col_weight}, Exposure={exp_weight}, SA={sa_weight}\")\n",
        "print(f\"üîß TRAINING HYPERPARAMETERS: LR={LR}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reuse datasets/loaders from Section 3 (defined)\n",
        "train_dataset = LOLDataset(train=True, crop_size=CROP_SIZE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True, prefetch_factor=2)\n",
        "\n",
        "val_dataset = LOLDataset(train=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Instantiate the new hybrid criterion with the defined weights\n",
        "hybrid_criterion = ZeroDCELoss_Hybrid_Supervised(recon_weight=recon_weight, percep_weight=percep_weight, spa_weight=spa_weight, tv_weight=tv_weight, col_weight=col_weight, exp_weight=exp_weight, sa_weight=sa_weight).to(device)\n",
        "\n",
        "# Instantiate the new supervised ZeroDCE architecture\n",
        "supervised_zerodce_model = ZeroDCE_Supervised_Architecture().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(supervised_zerodce_model.parameters(), lr=LR, weight_decay=1e-5)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Reusing SSIM for validation metrics\n",
        "cal_ssim_val = SSIM().to(device)\n",
        "\n",
        "# Reuse tracking from Section 3\n",
        "training_stats_supervised = {'epoch': [], 'loss': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    supervised_zerodce_model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for low, high in train_loader:\n",
        "        low, high = low.to(device), high.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            enhanced = supervised_zerodce_model(low) # Now returns only the enhanced image\n",
        "            loss = hybrid_criterion(enhanced, high)  # Now uses hybrid loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(supervised_zerodce_model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    if epoch % SHOW_EVERY == 0:\n",
        "        supervised_zerodce_model.eval()\n",
        "        psnr_list, ssim_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for v_low, v_high in val_loader:\n",
        "                v_low, v_high = v_low.to(device), v_high.to(device)\n",
        "                v_enhanced = supervised_zerodce_model(v_low).clamp(0,1) # Model now returns only enhanced image\n",
        "                ssim_list.append(cal_ssim_val(v_enhanced, v_high).item())\n",
        "                v_enhanced_np = v_enhanced.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                v_high_np = v_high.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                psnr_list.append(psnr(v_high_np, v_enhanced_np, data_range=1.0))\n",
        "\n",
        "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
        "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
        "\n",
        "        training_stats_supervised['epoch'].append(epoch)\n",
        "        training_stats_supervised['loss'].append(avg_loss)\n",
        "        training_stats_supervised['psnr'].append(avg_psnr)\n",
        "        training_stats_supervised['ssim'].append(avg_ssim)\n",
        "\n",
        "        print(f\"Epoch {epoch:2d}/{EPOCHS} | Loss: {avg_loss:.4f} | Test PSNR: {avg_psnr:.2f} | Test SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        # Reuse visualization from Section 3\n",
        "        with torch.no_grad():\n",
        "            idx = random.randint(0, len(all_low_paths)-1)\n",
        "            low_img = Image.open(all_low_paths[idx]).convert(\"RGB\")\n",
        "            high_img = Image.open(all_high_paths[idx]).convert(\"RGB\")\n",
        "            input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "            enhanced_tensor = supervised_zerodce_model(input_tensor).clamp(0,1).squeeze(0).cpu()\n",
        "            enhanced_img = T.ToPILImage()(enhanced_tensor)\n",
        "\n",
        "            plt.figure(figsize=(15,5))\n",
        "            plt.subplot(1,3,1); plt.title(\"Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "            plt.subplot(1,3,2); plt.title(\"Ground Truth\"); plt.imshow(high_img); plt.axis('off')\n",
        "            plt.subplot(1,3,3); plt.title(f\"Epoch {epoch} (PSNR {avg_psnr:.2f})\"); plt.imshow(enhanced_img); plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Save the model (reuse dict from notebook)\n",
        "trained_models['ZeroDCE_Supervised_Hybrid'] = supervised_zerodce_model\n",
        "\n",
        "# New: Final Summary and Plots\n",
        "if training_stats_supervised['psnr']:\n",
        "    final_avg_psnr = sum(training_stats_supervised['psnr']) / len(training_stats_supervised['psnr'])\n",
        "    final_avg_ssim = sum(training_stats_supervised['ssim']) / len(training_stats_supervised['ssim'])\n",
        "    print(f\"\\nFinal Average PSNR: {final_avg_psnr:.2f}\")\n",
        "    print(f\"Final Average SSIM: {final_avg_ssim:.4f}\")\n",
        "\n",
        "    # Plotting Training History\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(training_stats_supervised['epoch'], training_stats_supervised['loss'], label='Loss', color='red')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(training_stats_supervised['epoch'], training_stats_supervised['psnr'], label='PSNR', color='green')\n",
        "    plt.title('Validation PSNR')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(training_stats_supervised['epoch'], training_stats_supervised['ssim'], label='SSIM', color='blue')\n",
        "    plt.title('Validation SSIM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Hybrid ZeroDCE (Dual Loss: ZeroDCE + Charbonnier)\n",
        "\n",
        "### Quantitative Metrics (Test Set)\n",
        "- **Average PSNR**: 17.05  \n",
        "- **Average SSIM**: 0.5552  \n",
        "\n",
        "### Training Loss Observations\n",
        "The loss curve shows steady convergence, starting high and declining smoothly over epochs. This indicates stable optimization and progressive improvement in the model's ability to enhance low-light images.\n",
        "\n",
        "### Performance Insights\n",
        "The hybrid model outperforms the pure unsupervised ZeroDCE, demonstrating that incorporating supervised reconstruction (via Charbonnier loss) enhances pixel accuracy and perceptual quality while retaining ZeroDCE's illumination priors.\n",
        "\n",
        "However, it falls short of the supervised U-Net baseline, likely due to architectural differences: U-Net's encoder-decoder with skip connections excels at precise pixel-level transformations, whereas ZeroDCE's curve-based approach prioritizes global enhancement and naturalness over exact reconstruction. This trade-off makes the hybrid a balanced alternative for scenarios with limited paired data."
      ],
      "metadata": {
        "id": "tMR16wROJFpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 2: Hybrid_ZeroDCE_Only_Charbonnier\n",
        "\n",
        "This model uses ZeroDCE architecture with only Charbonnier loss"
      ],
      "metadata": {
        "id": "uwUfoXcaoxem"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6352c45f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "from piqa import SSIM\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# =============================================================================\n",
        "# üìå 4. Model 4: ZeroDCE Architecture with only Charbonnier Loss (Pure Supervised)\n",
        "# =============================================================================\n",
        "\n",
        "# Reuse ZeroDCE_Supervised_Architecture (already defined)\n",
        "# class ZeroDCE_Supervised_Architecture(ZeroDCE_Unsupervised_Model):\n",
        "#     def __init__(self):\n",
        "#         super(ZeroDCE_Supervised_Architecture, self).__init__()\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         _, enhanced_image, _ = super().forward(x)\n",
        "#         return enhanced_image\n",
        "\n",
        "# Reuse CharbonnierLoss (already defined)\n",
        "# class CharbonnierLoss(nn.Module):\n",
        "#     def __init__(self, eps=1e-3):\n",
        "#         super(CharbonnierLoss, self).__init__()\n",
        "#         self.eps = eps\n",
        "#\n",
        "#     def forward(self, x, y):\n",
        "#         diff = x - y\n",
        "#         return torch.mean(torch.sqrt((diff * diff) + (self.eps * self.eps)))\n",
        "\n",
        "# Define the hyperparameters to match paper\n",
        "LR = 0.0001 # CHANGED from 5e-5 to 0.0001\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 8 # CHANGED from 16 to 8\n",
        "CROP_SIZE = 256\n",
        "SHOW_EVERY = 10\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"STARTING TRAINING: MODEL 4 (ZeroDCE Arch + Pure Charbonnier Loss)\")\n",
        "print(f\"üîß TRAINING HYPERPARAMETERS: LR={LR}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Reuse datasets/loaders\n",
        "train_dataset = LOLDataset(train=True, crop_size=CROP_SIZE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True, prefetch_factor=2)\n",
        "\n",
        "val_dataset = LOLDataset(train=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Instantiate the supervised ZeroDCE architecture\n",
        "supervised_zerodce_model_pure_charbonnier = ZeroDCE_Supervised_Architecture().to(device)\n",
        "\n",
        "# The criterion is purely Charbonnier Loss\n",
        "criterion_pure_charbonnier = CharbonnierLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(supervised_zerodce_model_pure_charbonnier.parameters(), lr=LR, weight_decay=1e-5)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "# Reusing SSIM for validation metrics\n",
        "cal_ssim_val = SSIM().to(device)\n",
        "\n",
        "# Reuse tracking from Section 3\n",
        "training_stats_pure_charbonnier = {'epoch': [], 'loss': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    supervised_zerodce_model_pure_charbonnier.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for low, high in train_loader:\n",
        "        low, high = low.to(device), high.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            enhanced = supervised_zerodce_model_pure_charbonnier(low)\n",
        "            loss = criterion_pure_charbonnier(enhanced, high)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(supervised_zerodce_model_pure_charbonnier.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    if epoch % SHOW_EVERY == 0:\n",
        "        supervised_zerodce_model_pure_charbonnier.eval()\n",
        "        psnr_list, ssim_list = [], []\n",
        "        with torch.no_grad():\n",
        "            for v_low, v_high in val_loader:\n",
        "                v_low, v_high = v_low.to(device), v_high.to(device)\n",
        "                v_enhanced = supervised_zerodce_model_pure_charbonnier(v_low).clamp(0,1)\n",
        "                ssim_list.append(cal_ssim_val(v_enhanced, v_high).item())\n",
        "                v_enhanced_np = v_enhanced.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                v_high_np = v_high.squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "                psnr_list.append(psnr(v_high_np, v_enhanced_np, data_range=1.0))\n",
        "\n",
        "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
        "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
        "\n",
        "        training_stats_pure_charbonnier['epoch'].append(epoch)\n",
        "        training_stats_pure_charbonnier['loss'].append(avg_loss)\n",
        "        training_stats_pure_charbonnier['psnr'].append(avg_psnr)\n",
        "        training_stats_pure_charbonnier['ssim'].append(avg_ssim)\n",
        "\n",
        "        print(f\"Epoch {epoch:2d}/{EPOCHS} | Loss: {avg_loss:.4f} | Test PSNR: {avg_psnr:.2f} | Test SSIM: {avg_ssim:.4f}\")\n",
        "\n",
        "        # Reuse visualization from Section 3\n",
        "        with torch.no_grad():\n",
        "            rand_idx = random.randint(0, len(val_dataset) - 1)\n",
        "            low_img_sample, high_img_sample = val_dataset[rand_idx]\n",
        "            input_tensor_sample = low_img_sample.unsqueeze(0).to(device)\n",
        "\n",
        "            enhanced_tensor = supervised_zerodce_model_pure_charbonnier(input_tensor_sample).clamp(0,1).squeeze(0).cpu()\n",
        "            enhanced_img = T.ToPILImage()(enhanced_tensor)\n",
        "\n",
        "            plt.figure(figsize=(15,5))\n",
        "            plt.subplot(1,3,1); plt.title(\"Input\"); plt.imshow(T.ToPILImage()(low_img_sample)); plt.axis('off')\n",
        "            plt.subplot(1,3,2); plt.title(\"Ground Truth\"); plt.imshow(T.ToPILImage()(high_img_sample)); plt.axis('off')\n",
        "            plt.subplot(1,3,3); plt.title(f\"Epoch {epoch} (PSNR {avg_psnr:.2f})\"); plt.imshow(enhanced_img); plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# Save the model\n",
        "trained_models['ZeroDCE_Supervised_Pure_Charbonnier'] = supervised_zerodce_model_pure_charbonnier\n",
        "\n",
        "# Final Summary and Plots\n",
        "if training_stats_pure_charbonnier['psnr']:\n",
        "    final_avg_psnr = sum(training_stats_pure_charbonnier['psnr']) / len(training_stats_pure_charbonnier['psnr'])\n",
        "    final_avg_ssim = sum(training_stats_pure_charbonnier['ssim']) / len(training_stats_pure_charbonnier['ssim'])\n",
        "    print(f\"\\nFinal Average PSNR: {final_avg_psnr:.2f}\")\n",
        "    print(f\"Final Average SSIM: {final_avg_ssim:.4f}\")\n",
        "\n",
        "    # Plotting Training History\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(training_stats_pure_charbonnier['epoch'], training_stats_pure_charbonnier['loss'], label='Loss', color='red')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(training_stats_pure_charbonnier['epoch'], training_stats_pure_charbonnier['psnr'], label='PSNR', color='green')\n",
        "    plt.title('Validation PSNR')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(training_stats_pure_charbonnier['epoch'], training_stats_pure_charbonnier['ssim'], label='SSIM', color='blue')\n",
        "    plt.title('Validation SSIM')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Hybrid ZeroDCE Variant 2: Supervised (Charbonnier Loss Only)\n",
        "\n",
        "### Quantitative Metrics (Test Set)\n",
        "- **Average PSNR**: 16.74  \n",
        "- **Average SSIM**: 0.4993  \n",
        "\n",
        "### Training Loss Observations\n",
        "Similar to Variant 1, the loss curve generally decreases over epochs, indicating effective learning. However, PSNR and SSIM on the validation set exhibit more fluctuations compared to the dual-loss hybrid, suggesting reduced training stability without the original ZeroDCE priors.\n",
        "\n",
        "### Performance Insights\n",
        "This variant outperforms the pure unsupervised ZeroDCE, confirming that adding supervised reconstruction guidance (Charbonnier loss) improves accuracy and quality even on the ZeroDCE architecture.\n",
        "\n",
        "However, it underperforms both the supervised U-Net baseline and the dual-loss hybrid (PSNR 17.05 / SSIM 0.5552 for Variant 1). The increased volatility in metrics implies that removing the ZeroDCE non-reference losses eliminates useful regularization, leading to less consistent convergence. This highlights the value of hybrid objectives: the original priors act as stabilizers, promoting smoother optimization and slightly better overall results by balancing perceptual naturalness with pixel-level fidelity."
      ],
      "metadata": {
        "id": "7MBvW1DMSsx-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZW_lDDkg9e1"
      },
      "source": [
        "# üìå 5. Comparison of the Four Models\n",
        "\n",
        "This section evaluates the performance of the four trained models on the test set:\n",
        "1. U-Net with Charbonnier loss (supervised)\n",
        "2. ZeroDCE with original loss (unsupervised)\n",
        "3. Hybrid ZeroDCE with dual loss (ZeroDCE + Charbonnier)\n",
        "4. Hybrid ZeroDCE with Charbonnier loss only (supervised)\n",
        "\n",
        "The code below computes quantitative metrics (PSNR/SSIM) and generates visual comparisons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca14cf70"
      },
      "source": [
        "# Rename Model 1 and Model 4 for clarity in comparison\n",
        "if 'ZeroDCE_Supervised_Hybrid' in trained_models:\n",
        "    trained_models['Hybrid_ZeroDCE_Two_Losses'] = trained_models.pop('ZeroDCE_Supervised_Hybrid')\n",
        "    print(\"Renamed 'ZeroDCE_Supervised_Hybrid' to 'Hybrid_ZeroDCE_Two_Losses'\")\n",
        "\n",
        "if 'ZeroDCE_Supervised_Pure_Charbonnier' in trained_models:\n",
        "    trained_models['Hybrid_ZeroDCE_Only_Charbonnier'] = trained_models.pop('ZeroDCE_Supervised_Pure_Charbonnier')\n",
        "    print(\"Renamed 'ZeroDCE_Supervised_Pure_Charbonnier' to 'Hybrid_ZeroDCE_Only_Charbonnier'\")\n",
        "\n",
        "# Ensure UNet_Charbonnier is loaded, as its state dict is saved separately.\n",
        "# Re-load or ensure the trained UNet model is in trained_models for comparison\n",
        "# (This logic is already robust in the comparison cell, but for completeness)\n",
        "if 'UNet_Charbonnier' not in trained_models:\n",
        "    # This part should ideally not run if the first supervised section was executed successfully\n",
        "    charbonnier_model = UNet(depth=MODEL_DEPTH).to(device)\n",
        "    charbonnier_model_path = \"zero_dce_charbonnier_pro.pth\"\n",
        "    if os.path.exists(charbonnier_model_path):\n",
        "        charbonnier_model.load_state_dict(torch.load(charbonnier_model_path))\n",
        "        charbonnier_model.eval()\n",
        "        trained_models['UNet_Charbonnier'] = charbonnier_model\n",
        "        print(\"Ensured UNet_Charbonnier is in trained_models.\")\n",
        "    else:\n",
        "        print(f\"Warning: {charbonnier_model_path} not found. UNet_Charbonnier might be missing from comparison.\")\n",
        "\n",
        "print(\"Current models in trained_models for comparison:\", list(trained_models.keys()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrqfbcQuTa64"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Assuming `df` is the DataFrame from the previous full evaluation cell\n",
        "# Define the list of models to include in the final comparison\n",
        "key_models_names = [\n",
        "    'UNet_Charbonnier',\n",
        "    'ZeroDCE_Unsupervised',\n",
        "    'Hybrid_ZeroDCE_Two_Losses',\n",
        "    'Hybrid_ZeroDCE_Only_Charbonnier'\n",
        "]\n",
        "\n",
        "# Filter the DataFrame to include only the key models\n",
        "# Ensure 'df' exists; if not, you might need to re-run the full comparison cell first.\n",
        "if 'df' in globals():\n",
        "    final_comparison_df = df.loc[key_models_names]\n",
        "    print(\"Final Evaluation Results of Key Models:\")\n",
        "    display(final_comparison_df)\n",
        "\n",
        "    # Determine best models based on the filtered results\n",
        "    best_psnr_key = final_comparison_df['Avg PSNR'].idxmax()\n",
        "    best_ssim_key = final_comparison_df['Avg SSIM'].idxmax()\n",
        "\n",
        "    print(f\"Best by PSNR: {best_psnr_key}\")\n",
        "    print(f\"Best by SSIM: {best_ssim_key}\")\n",
        "else:\n",
        "    print(\"Warning: 'df' DataFrame not found. Please ensure the full comparison cell has been run.\")\n",
        "\n",
        "# --- Visual Comparison of Key Models ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VISUAL COMPARISON OF KEY MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensure 'trained_models' global dictionary is available\n",
        "if 'trained_models' not in globals():\n",
        "    print(\"Error: 'trained_models' dictionary not found. Please ensure all models are trained and saved.\")\n",
        "else:\n",
        "    # Filter trained_models to include only the key models\n",
        "    key_trained_models = {name: trained_models[name] for name in key_models_names if name in trained_models}\n",
        "\n",
        "    if not key_trained_models:\n",
        "        print(\"Error: None of the key models were found in 'trained_models' for visual comparison.\")\n",
        "    else:\n",
        "        test_low_paths_eval15 = sorted((Path(\"/content/lol_dataset/eval15/low\")).glob(\"*.png\"))\n",
        "        test_high_paths_eval15 = sorted((Path(\"/content/lol_dataset/eval15/high\")).glob(\"*.png\"))\n",
        "\n",
        "        if len(test_low_paths_eval15) == 0:\n",
        "            print(\"‚ö†Ô∏è Error: No images found in /content/lol_dataset/eval15/low.\")\n",
        "        else:\n",
        "            idx = random.randint(0, len(test_low_paths_eval15) - 1)\n",
        "            print(f\"Testing on image index: {idx} / {len(test_low_paths_eval15)-1}\")\n",
        "\n",
        "            low_img = Image.open(test_low_paths_eval15[idx]).convert(\"RGB\")\n",
        "            high_img = Image.open(test_high_paths_eval15[idx]).convert(\"RGB\")\n",
        "            input_tensor = T.ToTensor()(low_img).unsqueeze(0).to(device)\n",
        "\n",
        "            plt.figure(figsize=(20, 5))\n",
        "            total_plots = len(key_trained_models) + 2 # Low-light Input, Ground Truth, and each key model\n",
        "\n",
        "            plt.subplot(1, total_plots, 1); plt.title(\"Low-light Input\"); plt.imshow(low_img); plt.axis('off')\n",
        "            plt.subplot(1, total_plots, 2); plt.title(\"Ground Truth\"); plt.imshow(high_img); plt.axis('off')\n",
        "\n",
        "            plot_idx = 3\n",
        "            # Sort models alphabetically by name for consistent plotting order\n",
        "            sorted_key_trained_models = dict(sorted(key_trained_models.items()))\n",
        "\n",
        "            for name, model in sorted_key_trained_models.items():\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    output = model(input_tensor)\n",
        "                    if isinstance(output, tuple):\n",
        "                         _, enhanced_out, _ = output\n",
        "                         enhanced = enhanced_out.clamp(0,1).squeeze(0).cpu()\n",
        "                    else:\n",
        "                         enhanced = output.clamp(0,1).squeeze(0).cpu()\n",
        "\n",
        "                    enhanced_img = T.ToPILImage()(enhanced)\n",
        "                    plt.subplot(1, total_plots, plot_idx)\n",
        "                    plt.title(f\"Enhanced ({name})\")\n",
        "                    plt.imshow(enhanced_img)\n",
        "                    plt.axis('off')\n",
        "                    plot_idx += 1\n",
        "\n",
        "            plt.suptitle(\"Visual Comparison on a Test Image (eval15 Dataset)\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f3bc4e9"
      },
      "source": [
        "# üìå 6. Final Analysis of Model Performance\n",
        "\n",
        "Based on quantitative metrics (PSNR and SSIM), training stability, and qualitative visual inspections, the four models exhibit distinct performance characteristics:\n",
        "\n",
        "1. **U-Net + Charbonnier (Supervised)**:  \n",
        "   Consistently achieves the highest PSNR (~19.5-20.2, SSIM ~0.76‚Äì0.78). Outputs closely match ground truth with accurate brightness, contrast, color fidelity, and detail recovery.  \n",
        "   *Reason*: U-Net's encoder-decoder with skip connections excels at pixel-level reconstruction, optimized directly via supervised loss.\n",
        "\n",
        "2. **ZeroDCE (Unsupervised)**:  \n",
        "   Yields the lowest scores (PSNR ~14.6‚Äì14.7, SSIM ~0.47‚Äì0.49). Enhances visibility and contrast but introduces color shifts, noise, and artifacts.  \n",
        "   *Reason*: Relies solely on non-reference priors without ground-truth guidance, limiting pixel accuracy despite perceptual improvements.\n",
        "\n",
        "3. **Hybrid ZeroDCE (Dual Loss: ZeroDCE + Charbonnier)**:  \n",
        "   Significantly improves over unsupervised ZeroDCE (PSNR ~17.0‚Äì17.3, SSIM ~0.55‚Äì0.57). Produces natural, bright outputs with reduced artifacts and stable training.  \n",
        "   *Reason*: Supervised reconstruction enhances precision, while original priors provide regularization for better perceptual quality and convergence.\n",
        "\n",
        "4. **Hybrid ZeroDCE (Charbonnier Only)**:  \n",
        "   Performs comparably but slightly worse (PSNR ~16.7‚Äì17.0, SSIM ~0.49‚Äì0.51), with more metric fluctuations indicating reduced stability. Outputs are similar but less consistent in color and texture.  \n",
        "   *Reason*: Fully supervised but lacks ZeroDCE priors, which act as stabilizers‚Äîhighlighting the value of hybrid regularization to avoid over-focusing on pixel errors.\n",
        "\n",
        "### Overall Conclusion\n",
        "Supervised approaches dominate in low-light enhancement, with U-Net + Charbonnier as the benchmark for accuracy. ZeroDCE shines in unsupervised scenarios but benefits greatly from hybridization, where the dual-loss variant offers the best balance of metrics, stability, and naturalness. These findings emphasize strategic loss design and architecture choice for optimal results."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}